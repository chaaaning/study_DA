{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1 - 4.반드시 알아야하는 파이토치 스킬\n",
    "\n",
    "---\n",
    "\n",
    "1. Tensor\n",
    "   - Scalar\n",
    "   - Vector\n",
    "   - Matrix\n",
    "   - Tensor\n",
    "2. Autograd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.텐서\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1.1.Scalar\n",
    "\n",
    "---\n",
    "\n",
    "- 말 그대로 양이 존재하는 상숫 값을 의미함.\n",
    "- 사칙 연산 수행.\n",
    "- 하나의 값을 표현할 때 1개의 수치로 표현.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) ::: tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sc1 = torch.tensor([1.])\n",
    "sc2 = torch.tensor([3.])\n",
    "print(f\"{sc1} ::: {sc2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_scalar : tensor([4.])\n",
      "sub_scalar : tensor([-2.])\n",
      "mul_scalar : tensor([3.])\n",
      "div_scalar : tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "# 단순 사칙연산\n",
    "print(f\"add_scalar : {sc1+sc2}\")\n",
    "print(f\"sub_scalar : {sc1-sc2}\")\n",
    "print(f\"mul_scalar : {sc1*sc2}\")\n",
    "print(f\"div_scalar : {sc1/sc2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_scalar tensor([4.])\n",
      "sub_scalar tensor([-2.])\n",
      "mul_scalar tensor([3.])\n",
      "div_scalar tensor([0.3333])\n"
     ]
    }
   ],
   "source": [
    "# torch 내장 함수\n",
    "print(f\"add_scalar {torch.add(sc1,sc2)}\")\n",
    "print(f\"sub_scalar {torch.sub(sc1,sc2)}\")\n",
    "print(f\"mul_scalar {torch.mul(sc1,sc2)}\")\n",
    "print(f\"div_scalar {torch.div(sc1,sc2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2.Vector\n",
    "\n",
    "---\n",
    "\n",
    "- 2개 이상 수치 표현\n",
    "- 말 그대로 요소가 있는 벡터\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "v1, v2 = torch.tensor([1., 2., 3.]), torch.tensor([4., 5., 6.])\n",
    "print(v1, v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_scalar : tensor([5., 7., 9.])\n",
      "sub_scalar : tensor([-3., -3., -3.])\n",
      "mul_scalar : tensor([ 4., 10., 18.])\n",
      "div_scalar : tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "# 단순 사칙연산\n",
    "print(f\"add_scalar : {v1+v2}\")\n",
    "print(f\"sub_scalar : {v1-v2}\")\n",
    "print(f\"mul_scalar : {v1*v2}\")\n",
    "print(f\"div_scalar : {v1/v2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_scalar tensor([5., 7., 9.])\n",
      "sub_scalar tensor([-3., -3., -3.])\n",
      "mul_scalar tensor([ 4., 10., 18.])\n",
      "div_scalar tensor([0.2500, 0.4000, 0.5000])\n",
      "dot_scalar 32.0\n"
     ]
    }
   ],
   "source": [
    "# torch 내장 함수\n",
    "print(f\"add_scalar {torch.add(v1,v2)}\")\n",
    "print(f\"sub_scalar {torch.sub(v1,v2)}\")\n",
    "print(f\"mul_scalar {torch.mul(v1,v2)}\")\n",
    "print(f\"div_scalar {torch.div(v1,v2)}\")\n",
    "print(f\"dot_scalar {torch.dot(v1,v2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3.Matrix\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]]) \n",
      " tensor([[5., 6.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "mat1, mat2 = torch.tensor(\n",
    "    [[1., 2.], [3., 4.]]), torch.tensor([[5., 6.], [7., 8.]])\n",
    "print(mat1, \"\\n\", mat2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_scalar : tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "sub_scalar : tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "mul_scalar : tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "div_scalar : tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 단순 사칙연산\n",
    "print(f\"add_scalar : {mat1+mat2}\")\n",
    "print(f\"sub_scalar : {mat1-mat2}\")\n",
    "print(f\"mul_scalar : {mat1*mat2}\")\n",
    "print(f\"div_scalar : {mat1/mat2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_scalar tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "sub_scalar tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "mul_scalar tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "div_scalar tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n",
      "mat_mul_scalar tensor([[19., 22.],\n",
      "        [43., 50.]])\n"
     ]
    }
   ],
   "source": [
    "# torch 내장 함수\n",
    "print(f\"add_scalar {torch.add(mat1,mat2)}\")\n",
    "print(f\"sub_scalar {torch.sub(mat1,mat2)}\")\n",
    "print(f\"mul_scalar {torch.mul(mat1,mat2)}\")\n",
    "print(f\"div_scalar {torch.div(mat1,mat2)}\")\n",
    "print(f\"mat_mul_scalar {torch.matmul(mat1,mat2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4.Tensor\n",
    "\n",
    "---\n",
    "\n",
    "![스칼라 벡터 행렬 텐서 이미지](https://miro.medium.com/max/1400/1*pUr-9ctuGamgjSwoW_KU-A.png)\n",
    "\n",
    "> 출처 : [Linear Algebra for Deep Learning](https://towardsdatascience.com/linear-algebra-for-deep-learning-506c19c0d6fa)\n",
    "\n",
    "- 위와 같이 텐서는 2차원 이상의 배열이라 표현 할 수 있음\n",
    "- 즉 사칙 연산이 위치에 맞게끔 이루어짐\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n",
      "\n",
      "tensor([[[ 9., 10.],\n",
      "         [11., 12.]],\n",
      "\n",
      "        [[13., 14.],\n",
      "         [15., 16.]]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]])\n",
    "print(tensor1)\n",
    "print()\n",
    "tensor2 = torch.tensor([[[9., 10.], [11., 12.]], [[13., 14.], [15., 16.]]])\n",
    "print(tensor2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10., 12.],\n",
      "         [14., 16.]],\n",
      "\n",
      "        [[18., 20.],\n",
      "         [22., 24.]]])\n",
      "tensor([[[-8., -8.],\n",
      "         [-8., -8.]],\n",
      "\n",
      "        [[-8., -8.],\n",
      "         [-8., -8.]]])\n",
      "tensor([[[  9.,  20.],\n",
      "         [ 33.,  48.]],\n",
      "\n",
      "        [[ 65.,  84.],\n",
      "         [105., 128.]]])\n",
      "tensor([[[0.1111, 0.2000],\n",
      "         [0.2727, 0.3333]],\n",
      "\n",
      "        [[0.3846, 0.4286],\n",
      "         [0.4667, 0.5000]]])\n"
     ]
    }
   ],
   "source": [
    "# 단순 사칙연산\n",
    "print(tensor1+tensor2)  # add\n",
    "print(tensor1-tensor2)  # sub\n",
    "print(tensor1*tensor2)  # multiple\n",
    "print(tensor1/tensor2)  # divide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[10., 12.],\n",
      "         [14., 16.]],\n",
      "\n",
      "        [[18., 20.],\n",
      "         [22., 24.]]])\n",
      "tensor([[[-8., -8.],\n",
      "         [-8., -8.]],\n",
      "\n",
      "        [[-8., -8.],\n",
      "         [-8., -8.]]])\n",
      "tensor([[[  9.,  20.],\n",
      "         [ 33.,  48.]],\n",
      "\n",
      "        [[ 65.,  84.],\n",
      "         [105., 128.]]])\n",
      "tensor([[[0.1111, 0.2000],\n",
      "         [0.2727, 0.3333]],\n",
      "\n",
      "        [[0.3846, 0.4286],\n",
      "         [0.4667, 0.5000]]])\n",
      "tensor([[[ 31.,  34.],\n",
      "         [ 71.,  78.]],\n",
      "\n",
      "        [[155., 166.],\n",
      "         [211., 226.]]])\n"
     ]
    }
   ],
   "source": [
    "# torch 내장 함수\n",
    "print(torch.add(tensor1, tensor2))\n",
    "print(torch.sub(tensor1, tensor2))\n",
    "print(torch.mul(tensor1, tensor2))\n",
    "print(torch.div(tensor1, tensor2))\n",
    "print(torch.matmul(tensor1, tensor2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.Autograd\n",
    "\n",
    "---\n",
    "\n",
    "파이토치를 이용하여 코드를 작성할 때 Back Propagation을 이용해 파라미터를 업데이트하는 방법은 Autograd 방식으로 쉽게 구현할 수 있도록 설정 되어 있음  \n",
    "간단한 딥러닝 모델을 설계하고 방정식 내에 존재하는 파라미터 업데이트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 파이썬 실행환경에 맞게끔 장비를 설정\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "BATCH_SIZE = 64      # 딥러닝 모델에서 파라미터를 업데이트할 때 계산되는 데이터의 개수\n",
    "INPUT_SIZE = 1000    # 딥러닝 모델의 Input 크기이자 입력층의 노드 수\n",
    "HIDDEN_SIZE = 100    # Input을 다수의 파라미터를 이용해 계산한 결과에 한 번 더 계산되는 파라미터 수\n",
    "OUTPUT_SIZE = 10     # 딥러닝 모델에서 최종으로 출력되는 값의 벡터 크기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4종 파라미터에 대한 구체적인 내용**\n",
    "\n",
    "1. **`BATCH_SIZE`**\n",
    "   - 딥러닝 모델에서 파라미터를 업데이트할 때 계산되는 데이터 개수\n",
    "   - `BATCH_SIZE`의 수 만큼 데이터를 이용해 Output 계산\n",
    "   - `BATCH_SIZE`의 수 만큼 출력된 결과값에 대한 오차값을 계산\n",
    "   - `BATCH_SIZE`의 수 만큼 계산된 오차값을 평균 내어 Back Propagation을 적용하고 이를 바탕으로 파라미터를 업데이트\n",
    "   - 예제에서는 Input으로 이용되는 데이터가 64개  \n",
    "<br>\n",
    "\n",
    "2. **`INPUT_SIZE`**\n",
    "   - 딥러닝 모델에서의 Input의 크기이자 입력층의 노드 수\n",
    "   - 단순히 입력 데이터의 수\n",
    "   - 예제에서는 입력층 노드 수가 1000개\n",
    "   - `BATCH_SIZE`와 연계된 해석은 1000 크기의 벡터 값을 64개 이용한다는 뜻\n",
    "   - 이를 shape으로 설명하자면 (64, 1000)  \n",
    "<br>\n",
    "\n",
    "3. **`HIDDEN_SIZE`**\n",
    "   - 딥러닝 모델에서 Input을 다수의 파라미터를 이용해 계산한 결과에 한 번 더 계산되는 파라미터 수\n",
    "   - 입력층에서 은닉층으로 전달됐을 때 은닉층의 노드 수\n",
    "   - 예제 기준으로 (64, 1000)의 Input들이 (1000,100) 의 행렬과 행렬곱을 계산  \n",
    "<br>\n",
    "\n",
    "4. **`OUTPUT_SIZE`**\n",
    "   - 딥러닝 모델에서 최종으로 출력되는 값의 벡터의 크기\n",
    "   - 보통 Output의 크기는 비교하고자 하는 레이블의 크기와 동일하게 설정\n",
    "   - 예를 들어 10개의 레이블로 분류하려면 10짜리 One-Hot Encoding을 이용하기 때문에 `OUTPUT_SIZE`를 10으로 맞추기도 하고, 5 크기의 벡터 값에 대한 MSE를 계산하기 위해 5로 맞추기도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(BATCH_SIZE,\n",
    "                INPUT_SIZE,\n",
    "                device=DEVICE,\n",
    "                dtype=torch.float,\n",
    "                requires_grad=False)\n",
    "y = torch.randn(BATCH_SIZE,\n",
    "                OUTPUT_SIZE,\n",
    "                device=DEVICE,\n",
    "                dtype=torch.float,\n",
    "                requires_grad=False)\n",
    "w1 = torch.randn(INPUT_SIZE,\n",
    "                 HIDDEN_SIZE,\n",
    "                 device=DEVICE,\n",
    "                 dtype=torch.float,\n",
    "                 requires_grad=True)\n",
    "w2 = torch.randn(HIDDEN_SIZE,\n",
    "                 OUTPUT_SIZE,\n",
    "                 device=DEVICE,\n",
    "                 dtype=torch.float,\n",
    "                 requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input/Output 설정**\n",
    "\n",
    "- `torch.randn` : 평균이 0, 표준편차가 1인 정규분포에서 샘플링한 값 (데이터를 만들어 내는 의미)\n",
    "- `x` : Input을 설정\n",
    "    ```text\n",
    "    BATCH_SIZE(64), INPUT_SIZE(1000) \n",
    "        -> 크기 1000짜리의 벡터를 64개 만듦, x는 (64, 1000) 모양의 데이터가 생성\n",
    "    device = DEVICE\n",
    "        -> 미리 설정한 DEVICE (cuda or cpu)\n",
    "    requires_grad = False\n",
    "        -> Input Data이므로 Gradient를 계산할 필요 없음\n",
    "        -> Gradient는 파라미터를 업데이트 하기위해 계산하는 것이기 때문\n",
    "    ```\n",
    "- `y` : Output을 설정\n",
    "    ```TEXT\n",
    "    BATCH_SIZE(64), OUTPUT_SIZE(10)\n",
    "        -> 크기 10짜리의 벡터를 64개 만듦, y는 (64, 10) 모양의 데이터가 생성\n",
    "    ((Input 설정과 동일))\n",
    "    ```\n",
    "<br>\n",
    "\n",
    "**파라미터 설정**\n",
    "\n",
    "- `w1` : 업데이트 할 파라미터\n",
    "    ```text\n",
    "    INPUT_SIZE(1000), HIDDEN_SIZE(100)\n",
    "        -> INPUT 데이터와 행렬곱을 수행하여 HIDDEN의 개수로 결과 값 산출\n",
    "    requires_grad = True\n",
    "        -> Gradient 계산이 들어가므로 True 설정\n",
    "    ((Input/Output 설정과 동일))\n",
    "    ```\n",
    "- `w2` : 업데이트 할 파라미터\n",
    "    ```text\n",
    "    HIDDEN_SIZE(100), OUTPUT_SIZE(10)\n",
    "        -> 최종 라벨 결과가 10개이고 HIDDEN LAYER 이후 가산되므로 (100, 10)\n",
    "        -> x와 w1의 결과 (64, 1000)*(1000, 100) = (64, 100)\n",
    "        -> w2의 계산 결과 (64, 100)*(100, 10) = (64, 10)\n",
    "    ((w1 설정과 동일))\n",
    "    ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  100 \t Loss:  525.343994140625\n",
      "Iteration:  200 \t Loss:  3.517630100250244\n",
      "Iteration:  300 \t Loss:  0.05774771422147751\n",
      "Iteration:  400 \t Loss:  0.0014957755338400602\n",
      "Iteration:  500 \t Loss:  0.00015826108574401587\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(1, 501):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    if t % 100 == 0:\n",
    "        print(\"Iteration: \", t, \"\\t\", \"Loss: \", loss.item())\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code line 해석**\n",
    "\n",
    "```python\n",
    "learning_rate = 1e-6\n",
    "```\n",
    "- 파라미터 업데이트 시, Gradient를 계산한 결과값에 1보다 작은 값을 곱해 업데이트\n",
    "- 경사 하강에 변화를 주는 수치로 이해하면 될 듯\n",
    "- 딥러닝 모델에서 중요한 하이퍼 파라미터\n",
    "<br>\n",
    "\n",
    "```python\n",
    "y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "```\n",
    "- `y_pred`는 결과값이고, Input인 x와 중간층 w1의 행렬곱에 clamp 활성화 함수를 적용하고 해당 결과와 w2의 행렬곱을 수행\n",
    "- `.clamp(min=0)`\n",
    "    $\n",
    "    y_i=\n",
    "        \\begin{cases}\n",
    "        \\min & \\mathrm{if}\\;x_i<\\min \\\\\n",
    "        x_i, & \\mathrm{if}\\;\\min \\leq x_i \\leq \\max \\\\\n",
    "        \\max & \\mathrm{if}\\;x_i>\\max \n",
    "        \\end{cases}\n",
    "    $\n",
    "    - 최소값이 0으로 설정되었기 때문에 ReLU()와 같은 역할을 함\n",
    "<br>\n",
    "\n",
    "```python\n",
    "loss = (y_pred-y).pow(2).sum()\n",
    "```\n",
    "- 예측값과 실제값의 오차를 계산\n",
    "- `pow(k)`는 k제곱을 취하는 함수로 제곱 오차의 합을 loss로 설정\n",
    "<br>\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "- 계산된 Loss 값에 대해 `backward()` 메서드를 이용하여 각 파라미터 값에 대한 Gradient를 계산하고 이를 통해 Back Propagation을 진행\n",
    "- PyTorch 내에 구현된 Back Propagation\n",
    "<br>\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "```\n",
    "- 각 파라미터 값에 대해 Gradient를 계산한 결과를 이용하여 파라미터 값을 업데이트할 때는 해당 시점의 Gradient 값을 고정한 후 업데이트 진행\n",
    "- 코드가 실행되는 시점의 Gradient값을 고정한다는 의미\n",
    "<br>\n",
    "\n",
    "```python\n",
    "w1 -= learning_rate * w1.grad\n",
    "w2 -= learning_rate * w2.grad\n",
    "```\n",
    "- `w1의 Gradient`와 `learning_rate`를 곱한 값을 기존 `w1`에서 감산\n",
    "- 음수를 이용하는 이유는 Loss값이 최소로 계산될 수 있는 파라미터 값을 찾기 위해 Gradient 값에 대한 반대 방향으로 계산\n",
    "- 'w2'도 동일하게 Gradient Decent\n",
    "<br>\n",
    "\n",
    "```python\n",
    "w1.grad.zero_()\n",
    "w2.grad.zero_()\n",
    "```\n",
    "- 앞 과정에서 각 파라미터 값을 업데이트했다면 각 파라미터 값을 Gradient를 초기화하여 다음 loop를 수행할 수 있도록 Gradient 값을 0으로 설정\n",
    "- 다음 Backpropagation을 진행할 때 Gradient값을 `loss.backward()`로 새로 계산하기 때문\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8aee22797df4b788837c069f0ef8574f3e7e1be82db3fea6d988500a32afa623"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('dl_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
